{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54bef82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Using cached textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting pyahocorasick\n",
      "  Downloading pyahocorasick-2.0.0-cp39-cp39-win_amd64.whl (39 kB)\n",
      "Collecting anyascii\n",
      "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
      "     ------------------------------------- 289.9/289.9 kB 18.6 MB/s eta 0:00:00\n",
      "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b59a24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    53218.000000\n",
      "mean         0.843869\n",
      "std          0.362983\n",
      "min          0.000000\n",
      "25%          1.000000\n",
      "50%          1.000000\n",
      "75%          1.000000\n",
      "max          1.000000\n",
      "Name: Sentiment, dtype: float64\n",
      "1    44909\n",
      "0     8309\n",
      "Name: Sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"datawithsentiment.csv\")\n",
    "\n",
    "print(df['Sentiment'].describe())\n",
    "\n",
    "print(df['Sentiment'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c2dc252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idex                  0\n",
      "company               0\n",
      "location          17477\n",
      "dates                 0\n",
      "job-title             0\n",
      "summary               0\n",
      "pros                  0\n",
      "cons                  1\n",
      "advice-to-mgmt    22809\n",
      "overall               0\n",
      "rating1               0\n",
      "rating2               0\n",
      "rating3               0\n",
      "rating4               0\n",
      "rating5               0\n",
      "helpful-count         0\n",
      "Tech                  0\n",
      "Sentiment             0\n",
      "dtype: int64\n",
      "idex               int64\n",
      "company           object\n",
      "location          object\n",
      "dates             object\n",
      "job-title         object\n",
      "summary           object\n",
      "pros              object\n",
      "cons              object\n",
      "advice-to-mgmt    object\n",
      "overall            int64\n",
      "rating1            int64\n",
      "rating2            int64\n",
      "rating3            int64\n",
      "rating4            int64\n",
      "rating5            int64\n",
      "helpful-count      int64\n",
      "Tech               int64\n",
      "Sentiment          int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum()) \n",
    "print(data.dtypes) \n",
    "\n",
    "data = data[data['summary'].apply(lambda x: type(x) == str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac5aa333",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\TracyCui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\TracyCui\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import contractions\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "data = pd.read_csv('datawithsentiment.csv')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    text = ' '.join(lemmatizer.lemmatize(word) for word in text.split())\n",
    "    return text\n",
    "\n",
    "data['summary'] = data['summary'].apply(preprocess_text)\n",
    "\n",
    "data.to_csv('dataclean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59377c6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
